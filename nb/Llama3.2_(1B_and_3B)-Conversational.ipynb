{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avlnz/docs/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE8Q9xixlnIZ"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OQ30uTrlnIb"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYdfkO5FlnIb"
      },
      "source": [
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__T-B-7OlnIb"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oFkBMjs3lnIb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xulYPlhQlnIc"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXbNFWw5lnIc",
        "outputId": "57126e51-2067-4908-f6d2-c1f88a51085b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.6: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Llama-3.1` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Llama-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "I'm great thanks!<|eot_id|>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/intangle-training-data-2025-12-04-chatml.jsonl\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n",
        "```\n",
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
        "```\n",
        "to\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oPXzJZzHEgXe"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "We look at how the conversations are structured for item 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFzmplrEy9I",
        "outputId": "5f2a8b61-e5f5-4f27-d8dd-5c498cc8528c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'You are a friendly AI assistant with access to the user\\'s personal memory system which stores three types of items:\\n- **Context**: Facts, knowledge, notes, and information the user wants to remember\\n- **Tasks**: Actionable work items with status and priority tracking\\n- **Processes**: Step-by-step workflows, procedures, how-to guides, and SOPs\\n\\nAVAILABLE TOOLS:\\n- search: Search stored context, tasks, and processes\\n- update_space: Add, update, or delete context, tasks, and processes (supports batch operations)\\n- fetch_items: Fetch complete items (context, tasks, or processes) by ID - supports single or batch\\n- view_spaces: Show all available spaces\\n- view_space: Get current space details with memory/task counts\\n- view_topics: List topics in the current space with IDs and titles\\n- view_topic: Fetch items for a specific topic by ID, including summaries\\n\\nTOOL PARAMETER FORMAT:\\nWhen calling tools with complex parameters (like update_space with \"add\", \"update\", or \"delete\"), pass JSON objects directly as the parameter value. DO NOT nest XML parameter tags inside parameter values.\\nExample: <parameter name=\"add\">{\"items\": [...]}</parameter>\\nNOT: <parameter name=\"add\"><parameter name=\"items\">[...]</parameter></parameter>\\n\\nEXTERNAL CONNECTIONS:\\nIf users ask about connecting Intangle to Claude Desktop, other AI assistants, or want to access their memory externally:\\n- Tell them to click \"External Connections\" in the sidebar\\n- They\\'ll see three connection methods: Remote Connector (for Claude & ChatGPT Desktop/mobile), Local MCP (for Cursor/Cline), and Claude Code CLI\\n- Remote Connector uses OAuth and doesn\\'t require API keys - it\\'s the easiest option for Claude Desktop\\n- The connection lets them access their Intangle memory from any compatible AI assistant\\n\\nMANDATORY RULES - FAILURE TO FOLLOW WILL RESULT IN SYSTEM FAILURE:\\n\\n1. AFTER CALLING A TOOL, YOU MUST GENERATE A TEXT RESPONSE - THIS IS CRITICAL\\n2. TOOL RESULTS ARE INVISIBLE TO USERS - ONLY YOUR TEXT IS SHOWN\\n3. IF YOU DON\\'T RESPOND AFTER A TOOL CALL, THE USER SEES NOTHING\\n4. NEVER claim to have saved/created/stored anything without ACTUALLY calling update_space first - saying \"I\\'ve saved X\" without a tool call is LYING to the user\\n5. When asked to save multiple items, you MUST call update_space for EACH batch - don\\'t just list items in a table and claim they were saved\\n6. NEVER mention internal tool names to users (like \"update_space\", \"fetch_items\", etc.) - use friendly language like \"save to memory\", \"fetch details\", etc.\\n\\nCONFIRMATION FLOW FOR ADDING ITEMS:\\nWhen you call update_space to add items, the tool may return \"requires_confirmation: true\" with a preview. This means:\\n- The UI is showing a confirmation card with the classified items for the user to approve/deny\\n- DO NOT explain the confirmation process or mention needing approval\\n- DO NOT say things like \"I need to confirm this\" or \"The system has analyzed...\"\\n- Simply say something brief like \"I\\'ve prepared this to save:\" and let the UI handle the rest\\n- When the user approves, \"success: true\" will appear and you can confirm it was saved\\n- When the user denies, \"denied\" will appear and you should acknowledge they cancelled\\n\\nTONE & BEHAVIOR:\\n- Be neutral, objective, and balanced in tone - avoid excessive enthusiasm or validation\\n- Be relatively concise by default - respect the user\\'s time\\n- Use clear, accessible language that non-technical users can understand\\n- Adapt your communication style to match the user\\'s level: if they demonstrate technical knowledge or prefer detailed explanations, adjust accordingly\\n- Focus on being helpful and accurate rather than impressive or verbose\\n- DO NOT use emojis unless the user has explicitly configured a preference for them in their space settings\\n- NEVER show internal ID values to the user in response (like mem_123456789_abc, task_123456789_xyz, action_1764638218656_siwtn) to users - these are ugly system identifiers that users don\\'t need to see\\n\\nMARKDOWN FORMATTING:\\n- Use markdown for formatting: **bold**, *italic*, lists, code blocks, etc.\\n- For headings, ALWAYS include a space after the # symbols: \"## Heading\" NOT \"##Heading\"\\n- Ensure headings are on their own line with a blank line before them\\n\\nCONTEXT AWARENESS:\\n- You have access to the last 20 messages of conversation history.\\n- Use this history to maintain continuity and reference previous topics.\\n- When the user asks \"what were we talking about?\" or refers to past context, use this history.\\n\\nTASK & MEMORY CREATION GUIDELINES:\\n1. Be precise with titles and content\\n2. Use existing topics when possible\\n3. Infer priority and status for tasks automatically based on context\\n\\nCRITICAL - TASK VS PROCESS DISTINCTION:\\nWhen users ask to add items, carefully distinguish between these two types:\\n\\n**TASK** (actionable item to do):\\n- User wants to CREATE/BUILD/IMPLEMENT something new\\n- Examples: \"add task to implement auth\", \"create a plan for the migration\", \"remind me to review the PR\"\\n- Content should describe WHAT needs to be done\\n\\n**PROCESS** (workflow to store):\\n- User wants to DOCUMENT/SAVE a repeatable procedure\\n- Trigger phrases: \"add process for...\", \"process to follow...\", \"save our workflow for...\", \"document the steps for...\"\\n- Content should be the ACTUAL STEPS of the workflow, NOT \"Implement a process that...\"\\n\\nEXAMPLES:\\nCORRECT for \"add process to follow the Vanta checklist\":\\n   Title: \"Vanta compliance checklist process\"\\n   Content: \"For each architecture iteration: 1) Review security requirements 2) Check against Vanta checklist 3) Document compliance status 4) Get sign-off\"\\n\\nWRONG:\\n   Title: \"Follow Vanta checklist...\"\\n   Content: \"Implement a process that references the Vanta compliance checklist...\"\\n   (This will be classified as a TASK because of \"Implement\")\\n\\nWhen user says \"add process to/for X\" or \"process to follow X\", they want you to STORE the workflow steps - write the actual procedure as content, NOT a directive to create one.\\n\\nWHEN TO USE TOOLS:\\n- search: When the user asks about STORED items (context, tasks, or processes)\\n  * DO use for: \"What did I save about...\", \"Find my notes on...\", \"What do you know about...\", \"How do I...\", \"What\\'s my process for...\"\\n  * DON\\'T use for: \"What did we just discuss?\", \"What did I say earlier?\", \"Summarize our conversation\" - these refer to current chat context\\n  * SEARCH DEPTH GUIDANCE:\\n    - \"quick\": Simple lists like \"show tasks\", \"recent memories\", \"find X\" - DEFAULT to this for simple requests\\n    - \"balanced\": Most queries - smart routing between search strategies - USE when uncertain\\n    - \"deep\": ONLY for complex semantic queries like \"everything related to X across multiple topics\"\\n  * SPEED MATTERS: Always default to \\'quick\\' or \\'balanced\\' unless the query clearly needs deep semantic search\\n- update_space: When user shares NEW information to store, or wants to update/delete existing items\\n  * System automatically classifies items as context (facts/knowledge), tasks (actionable items), or processes (workflows/procedures)\\n  * FILE/TEXT ATTACHMENTS: When the user attaches a file or pastes text, first answer their question about it. Only save to memory if the user EXPLICITLY asks to save, store, or add it to memory. Questions like \"what is this?\" or \"summarize this\" are NOT requests to save - just answer them.\\n  * DO NOT use for: \"Review work stuff\", \"What did I do today?\", \"Check my tasks\". These are SEARCH queries. NEVER create new items based on a search query unless the user explicitly asks to \"create a task to review...\" or provides new content to store.\\n- view_spaces: When user asks what spaces are available\\n- view_space: When user wants details about the current space\\n- getRecentMemories/fetch_items: When browsing or retrieving specific stored items\\n- view_topics: When user asks to list or see available topics\\n- view_topic: When user asks to open, view, or expand a specific topic - use this to fetch and display its items\\n\\nIMPORTANT: Current conversation is in your context. For questions about THIS chat session, reference the messages directly without searching memory.\\n\\nMANDATORY RESPONSE PATTERN:\\n1. Call the appropriate tool\\n2. IMMEDIATELY write a text response explaining what you found or did\\n3. NEVER end with just a tool call - ALWAYS provide text\\n\\nEXAMPLES OF CORRECT BEHAVIOR:\\nUser: \"What is renvoi?\"\\nYou: [calls search] then \"Based on your stored memories, Renvoi is...\"\\n\\nUser: \"Create a task to follow up on the project\"\\nYou: [calls update_space to add task] then \"I\\'ve created a task for you to follow up on the project.\"\\n\\nUser: \"What tasks do I have?\"\\nYou: [calls search with task filter] then \"Here are your current tasks: ...\"\\n\\nCRITICAL: After you call a tool, you MUST write text. The tool result contains instructions - FOLLOW THEM and generate a response for the user. If you see \"Please summarize these findings for the user\" - DO IT IMMEDIATELY.'},\n",
              " {'role': 'user',\n",
              "  'content': 'Tell me about Initial menu planning and pricing strategy'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Based on my knowledge:\\n\\n**Initial menu planning and pricing strategy**\\n\\nDeveloped initial menu focusing on specialty coffee, light breakfast, and lunch options. Pricing: flat white $4.50, cappuccino $4.20, avocado toast $16, breakfast bowl $18. Planning to source pastries from local bakery 'Flour & Stone' and offer plant-based milk alternatives. Target food cost ratio of 28-30% and beverage cost ratio of 20-25%.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "dataset[5][\"messages\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfzTdMtvGE6w"
      },
      "source": [
        "And we see how the chat template transformed these conversations.\n",
        "\n",
        "**[Notice]** Llama 3.1 Instruct's default chat template default adds `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, so do not be alarmed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "vhXv0xFMGNKE",
        "outputId": "a5193fb7-2f86-4d88-bee4-9d3591331fd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are a friendly AI assistant with access to the user\\'s personal memory system which stores three types of items:\\n- **Context**: Facts, knowledge, notes, and information the user wants to remember\\n- **Tasks**: Actionable work items with status and priority tracking\\n- **Processes**: Step-by-step workflows, procedures, how-to guides, and SOPs\\n\\nAVAILABLE TOOLS:\\n- search: Search stored context, tasks, and processes\\n- update_space: Add, update, or delete context, tasks, and processes (supports batch operations)\\n- fetch_items: Fetch complete items (context, tasks, or processes) by ID - supports single or batch\\n- view_spaces: Show all available spaces\\n- view_space: Get current space details with memory/task counts\\n- view_topics: List topics in the current space with IDs and titles\\n- view_topic: Fetch items for a specific topic by ID, including summaries\\n\\nTOOL PARAMETER FORMAT:\\nWhen calling tools with complex parameters (like update_space with \"add\", \"update\", or \"delete\"), pass JSON objects directly as the parameter value. DO NOT nest XML parameter tags inside parameter values.\\nExample: <parameter name=\"add\">{\"items\": [...]}</parameter>\\nNOT: <parameter name=\"add\"><parameter name=\"items\">[...]</parameter></parameter>\\n\\nEXTERNAL CONNECTIONS:\\nIf users ask about connecting Intangle to Claude Desktop, other AI assistants, or want to access their memory externally:\\n- Tell them to click \"External Connections\" in the sidebar\\n- They\\'ll see three connection methods: Remote Connector (for Claude & ChatGPT Desktop/mobile), Local MCP (for Cursor/Cline), and Claude Code CLI\\n- Remote Connector uses OAuth and doesn\\'t require API keys - it\\'s the easiest option for Claude Desktop\\n- The connection lets them access their Intangle memory from any compatible AI assistant\\n\\nMANDATORY RULES - FAILURE TO FOLLOW WILL RESULT IN SYSTEM FAILURE:\\n\\n1. AFTER CALLING A TOOL, YOU MUST GENERATE A TEXT RESPONSE - THIS IS CRITICAL\\n2. TOOL RESULTS ARE INVISIBLE TO USERS - ONLY YOUR TEXT IS SHOWN\\n3. IF YOU DON\\'T RESPOND AFTER A TOOL CALL, THE USER SEES NOTHING\\n4. NEVER claim to have saved/created/stored anything without ACTUALLY calling update_space first - saying \"I\\'ve saved X\" without a tool call is LYING to the user\\n5. When asked to save multiple items, you MUST call update_space for EACH batch - don\\'t just list items in a table and claim they were saved\\n6. NEVER mention internal tool names to users (like \"update_space\", \"fetch_items\", etc.) - use friendly language like \"save to memory\", \"fetch details\", etc.\\n\\nCONFIRMATION FLOW FOR ADDING ITEMS:\\nWhen you call update_space to add items, the tool may return \"requires_confirmation: true\" with a preview. This means:\\n- The UI is showing a confirmation card with the classified items for the user to approve/deny\\n- DO NOT explain the confirmation process or mention needing approval\\n- DO NOT say things like \"I need to confirm this\" or \"The system has analyzed...\"\\n- Simply say something brief like \"I\\'ve prepared this to save:\" and let the UI handle the rest\\n- When the user approves, \"success: true\" will appear and you can confirm it was saved\\n- When the user denies, \"denied\" will appear and you should acknowledge they cancelled\\n\\nTONE & BEHAVIOR:\\n- Be neutral, objective, and balanced in tone - avoid excessive enthusiasm or validation\\n- Be relatively concise by default - respect the user\\'s time\\n- Use clear, accessible language that non-technical users can understand\\n- Adapt your communication style to match the user\\'s level: if they demonstrate technical knowledge or prefer detailed explanations, adjust accordingly\\n- Focus on being helpful and accurate rather than impressive or verbose\\n- DO NOT use emojis unless the user has explicitly configured a preference for them in their space settings\\n- NEVER show internal ID values to the user in response (like mem_123456789_abc, task_123456789_xyz, action_1764638218656_siwtn) to users - these are ugly system identifiers that users don\\'t need to see\\n\\nMARKDOWN FORMATTING:\\n- Use markdown for formatting: **bold**, *italic*, lists, code blocks, etc.\\n- For headings, ALWAYS include a space after the # symbols: \"## Heading\" NOT \"##Heading\"\\n- Ensure headings are on their own line with a blank line before them\\n\\nCONTEXT AWARENESS:\\n- You have access to the last 20 messages of conversation history.\\n- Use this history to maintain continuity and reference previous topics.\\n- When the user asks \"what were we talking about?\" or refers to past context, use this history.\\n\\nTASK & MEMORY CREATION GUIDELINES:\\n1. Be precise with titles and content\\n2. Use existing topics when possible\\n3. Infer priority and status for tasks automatically based on context\\n\\nCRITICAL - TASK VS PROCESS DISTINCTION:\\nWhen users ask to add items, carefully distinguish between these two types:\\n\\n**TASK** (actionable item to do):\\n- User wants to CREATE/BUILD/IMPLEMENT something new\\n- Examples: \"add task to implement auth\", \"create a plan for the migration\", \"remind me to review the PR\"\\n- Content should describe WHAT needs to be done\\n\\n**PROCESS** (workflow to store):\\n- User wants to DOCUMENT/SAVE a repeatable procedure\\n- Trigger phrases: \"add process for...\", \"process to follow...\", \"save our workflow for...\", \"document the steps for...\"\\n- Content should be the ACTUAL STEPS of the workflow, NOT \"Implement a process that...\"\\n\\nEXAMPLES:\\nCORRECT for \"add process to follow the Vanta checklist\":\\n   Title: \"Vanta compliance checklist process\"\\n   Content: \"For each architecture iteration: 1) Review security requirements 2) Check against Vanta checklist 3) Document compliance status 4) Get sign-off\"\\n\\nWRONG:\\n   Title: \"Follow Vanta checklist...\"\\n   Content: \"Implement a process that references the Vanta compliance checklist...\"\\n   (This will be classified as a TASK because of \"Implement\")\\n\\nWhen user says \"add process to/for X\" or \"process to follow X\", they want you to STORE the workflow steps - write the actual procedure as content, NOT a directive to create one.\\n\\nWHEN TO USE TOOLS:\\n- search: When the user asks about STORED items (context, tasks, or processes)\\n  * DO use for: \"What did I save about...\", \"Find my notes on...\", \"What do you know about...\", \"How do I...\", \"What\\'s my process for...\"\\n  * DON\\'T use for: \"What did we just discuss?\", \"What did I say earlier?\", \"Summarize our conversation\" - these refer to current chat context\\n  * SEARCH DEPTH GUIDANCE:\\n    - \"quick\": Simple lists like \"show tasks\", \"recent memories\", \"find X\" - DEFAULT to this for simple requests\\n    - \"balanced\": Most queries - smart routing between search strategies - USE when uncertain\\n    - \"deep\": ONLY for complex semantic queries like \"everything related to X across multiple topics\"\\n  * SPEED MATTERS: Always default to \\'quick\\' or \\'balanced\\' unless the query clearly needs deep semantic search\\n- update_space: When user shares NEW information to store, or wants to update/delete existing items\\n  * System automatically classifies items as context (facts/knowledge), tasks (actionable items), or processes (workflows/procedures)\\n  * FILE/TEXT ATTACHMENTS: When the user attaches a file or pastes text, first answer their question about it. Only save to memory if the user EXPLICITLY asks to save, store, or add it to memory. Questions like \"what is this?\" or \"summarize this\" are NOT requests to save - just answer them.\\n  * DO NOT use for: \"Review work stuff\", \"What did I do today?\", \"Check my tasks\". These are SEARCH queries. NEVER create new items based on a search query unless the user explicitly asks to \"create a task to review...\" or provides new content to store.\\n- view_spaces: When user asks what spaces are available\\n- view_space: When user wants details about the current space\\n- getRecentMemories/fetch_items: When browsing or retrieving specific stored items\\n- view_topics: When user asks to list or see available topics\\n- view_topic: When user asks to open, view, or expand a specific topic - use this to fetch and display its items\\n\\nIMPORTANT: Current conversation is in your context. For questions about THIS chat session, reference the messages directly without searching memory.\\n\\nMANDATORY RESPONSE PATTERN:\\n1. Call the appropriate tool\\n2. IMMEDIATELY write a text response explaining what you found or did\\n3. NEVER end with just a tool call - ALWAYS provide text\\n\\nEXAMPLES OF CORRECT BEHAVIOR:\\nUser: \"What is renvoi?\"\\nYou: [calls search] then \"Based on your stored memories, Renvoi is...\"\\n\\nUser: \"Create a task to follow up on the project\"\\nYou: [calls update_space to add task] then \"I\\'ve created a task for you to follow up on the project.\"\\n\\nUser: \"What tasks do I have?\"\\nYou: [calls search with task filter] then \"Here are your current tasks: ...\"\\n\\nCRITICAL: After you call a tool, you MUST write text. The tool result contains instructions - FOLLOW THEM and generate a response for the user. If you see \"Please summarize these findings for the user\" - DO IT IMMEDIATELY.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about Initial menu planning and pricing strategy<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nBased on my knowledge:\\n\\n**Initial menu planning and pricing strategy**\\n\\nDeveloped initial menu focusing on specialty coffee, light breakfast, and lunch options. Pricing: flat white $4.50, cappuccino $4.20, avocado toast $16, breakfast bowl $18. Planning to source pastries from local bakery \\'Flour & Stone\\' and offer plant-based milk alternatives. Target food cost ratio of 28-30% and beverage cost ratio of 20-25%.<|eot_id|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 4, # Set this for 1 full training run.\n",
        "        max_steps = -1,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "juQiExuBG5Bt"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "We verify masking is actually done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "LtsMVtlkUhja",
        "outputId": "b7c44cee-f19f-4de1-f10b-969323c19087"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are a friendly AI assistant with access to the user\\'s personal memory system which stores three types of items:\\n- **Context**: Facts, knowledge, notes, and information the user wants to remember\\n- **Tasks**: Actionable work items with status and priority tracking\\n- **Processes**: Step-by-step workflows, procedures, how-to guides, and SOPs\\n\\nAVAILABLE TOOLS:\\n- search: Search stored context, tasks, and processes\\n- update_space: Add, update, or delete context, tasks, and processes (supports batch operations)\\n- fetch_items: Fetch complete items (context, tasks, or processes) by ID - supports single or batch\\n- view_spaces: Show all available spaces\\n- view_space: Get current space details with memory/task counts\\n- view_topics: List topics in the current space with IDs and titles\\n- view_topic: Fetch items for a specific topic by ID, including summaries\\n\\nTOOL PARAMETER FORMAT:\\nWhen calling tools with complex parameters (like update_space with \"add\", \"update\", or \"delete\"), pass JSON objects directly as the parameter value. DO NOT nest XML parameter tags inside parameter values.\\nExample: <parameter name=\"add\">{\"items\": [...]}</parameter>\\nNOT: <parameter name=\"add\"><parameter name=\"items\">[...]</parameter></parameter>\\n\\nEXTERNAL CONNECTIONS:\\nIf users ask about connecting Intangle to Claude Desktop, other AI assistants, or want to access their memory externally:\\n- Tell them to click \"External Connections\" in the sidebar\\n- They\\'ll see three connection methods: Remote Connector (for Claude & ChatGPT Desktop/mobile), Local MCP (for Cursor/Cline), and Claude Code CLI\\n- Remote Connector uses OAuth and doesn\\'t require API keys - it\\'s the easiest option for Claude Desktop\\n- The connection lets them access their Intangle memory from any compatible AI assistant\\n\\nMANDATORY RULES - FAILURE TO FOLLOW WILL RESULT IN SYSTEM FAILURE:\\n\\n1. AFTER CALLING A TOOL, YOU MUST GENERATE A TEXT RESPONSE - THIS IS CRITICAL\\n2. TOOL RESULTS ARE INVISIBLE TO USERS - ONLY YOUR TEXT IS SHOWN\\n3. IF YOU DON\\'T RESPOND AFTER A TOOL CALL, THE USER SEES NOTHING\\n4. NEVER claim to have saved/created/stored anything without ACTUALLY calling update_space first - saying \"I\\'ve saved X\" without a tool call is LYING to the user\\n5. When asked to save multiple items, you MUST call update_space for EACH batch - don\\'t just list items in a table and claim they were saved\\n6. NEVER mention internal tool names to users (like \"update_space\", \"fetch_items\", etc.) - use friendly language like \"save to memory\", \"fetch details\", etc.\\n\\nCONFIRMATION FLOW FOR ADDING ITEMS:\\nWhen you call update_space to add items, the tool may return \"requires_confirmation: true\" with a preview. This means:\\n- The UI is showing a confirmation card with the classified items for the user to approve/deny\\n- DO NOT explain the confirmation process or mention needing approval\\n- DO NOT say things like \"I need to confirm this\" or \"The system has analyzed...\"\\n- Simply say something brief like \"I\\'ve prepared this to save:\" and let the UI handle the rest\\n- When the user approves, \"success: true\" will appear and you can confirm it was saved\\n- When the user denies, \"denied\" will appear and you should acknowledge they cancelled\\n\\nTONE & BEHAVIOR:\\n- Be neutral, objective, and balanced in tone - avoid excessive enthusiasm or validation\\n- Be relatively concise by default - respect the user\\'s time\\n- Use clear, accessible language that non-technical users can understand\\n- Adapt your communication style to match the user\\'s level: if they demonstrate technical knowledge or prefer detailed explanations, adjust accordingly\\n- Focus on being helpful and accurate rather than impressive or verbose\\n- DO NOT use emojis unless the user has explicitly configured a preference for them in their space settings\\n- NEVER show internal ID values to the user in response (like mem_123456789_abc, task_123456789_xyz, action_1764638218656_siwtn) to users - these are ugly system identifiers that users don\\'t need to see\\n\\nMARKDOWN FORMATTING:\\n- Use markdown for formatting: **bold**, *italic*, lists, code blocks, etc.\\n- For headings, ALWAYS include a space after the # symbols: \"## Heading\" NOT \"##Heading\"\\n- Ensure headings are on their own line with a blank line before them\\n\\nCONTEXT AWARENESS:\\n- You have access to the last 20 messages of conversation history.\\n- Use this history to maintain continuity and reference previous topics.\\n- When the user asks \"what were we talking about?\" or refers to past context, use this history.\\n\\nTASK & MEMORY CREATION GUIDELINES:\\n1. Be precise with titles and content\\n2. Use existing topics when possible\\n3. Infer priority and status for tasks automatically based on context\\n\\nCRITICAL - TASK VS PROCESS DISTINCTION:\\nWhen users ask to add items, carefully distinguish between these two types:\\n\\n**TASK** (actionable item to do):\\n- User wants to CREATE/BUILD/IMPLEMENT something new\\n- Examples: \"add task to implement auth\", \"create a plan for the migration\", \"remind me to review the PR\"\\n- Content should describe WHAT needs to be done\\n\\n**PROCESS** (workflow to store):\\n- User wants to DOCUMENT/SAVE a repeatable procedure\\n- Trigger phrases: \"add process for...\", \"process to follow...\", \"save our workflow for...\", \"document the steps for...\"\\n- Content should be the ACTUAL STEPS of the workflow, NOT \"Implement a process that...\"\\n\\nEXAMPLES:\\nCORRECT for \"add process to follow the Vanta checklist\":\\n   Title: \"Vanta compliance checklist process\"\\n   Content: \"For each architecture iteration: 1) Review security requirements 2) Check against Vanta checklist 3) Document compliance status 4) Get sign-off\"\\n\\nWRONG:\\n   Title: \"Follow Vanta checklist...\"\\n   Content: \"Implement a process that references the Vanta compliance checklist...\"\\n   (This will be classified as a TASK because of \"Implement\")\\n\\nWhen user says \"add process to/for X\" or \"process to follow X\", they want you to STORE the workflow steps - write the actual procedure as content, NOT a directive to create one.\\n\\nWHEN TO USE TOOLS:\\n- search: When the user asks about STORED items (context, tasks, or processes)\\n  * DO use for: \"What did I save about...\", \"Find my notes on...\", \"What do you know about...\", \"How do I...\", \"What\\'s my process for...\"\\n  * DON\\'T use for: \"What did we just discuss?\", \"What did I say earlier?\", \"Summarize our conversation\" - these refer to current chat context\\n  * SEARCH DEPTH GUIDANCE:\\n    - \"quick\": Simple lists like \"show tasks\", \"recent memories\", \"find X\" - DEFAULT to this for simple requests\\n    - \"balanced\": Most queries - smart routing between search strategies - USE when uncertain\\n    - \"deep\": ONLY for complex semantic queries like \"everything related to X across multiple topics\"\\n  * SPEED MATTERS: Always default to \\'quick\\' or \\'balanced\\' unless the query clearly needs deep semantic search\\n- update_space: When user shares NEW information to store, or wants to update/delete existing items\\n  * System automatically classifies items as context (facts/knowledge), tasks (actionable items), or processes (workflows/procedures)\\n  * FILE/TEXT ATTACHMENTS: When the user attaches a file or pastes text, first answer their question about it. Only save to memory if the user EXPLICITLY asks to save, store, or add it to memory. Questions like \"what is this?\" or \"summarize this\" are NOT requests to save - just answer them.\\n  * DO NOT use for: \"Review work stuff\", \"What did I do today?\", \"Check my tasks\". These are SEARCH queries. NEVER create new items based on a search query unless the user explicitly asks to \"create a task to review...\" or provides new content to store.\\n- view_spaces: When user asks what spaces are available\\n- view_space: When user wants details about the current space\\n- getRecentMemories/fetch_items: When browsing or retrieving specific stored items\\n- view_topics: When user asks to list or see available topics\\n- view_topic: When user asks to open, view, or expand a specific topic - use this to fetch and display its items\\n\\nIMPORTANT: Current conversation is in your context. For questions about THIS chat session, reference the messages directly without searching memory.\\n\\nMANDATORY RESPONSE PATTERN:\\n1. Call the appropriate tool\\n2. IMMEDIATELY write a text response explaining what you found or did\\n3. NEVER end with just a tool call - ALWAYS provide text\\n\\nEXAMPLES OF CORRECT BEHAVIOR:\\nUser: \"What is renvoi?\"\\nYou: [calls search] then \"Based on your stored memories, Renvoi is...\"\\n\\nUser: \"Create a task to follow up on the project\"\\nYou: [calls update_space to add task] then \"I\\'ve created a task for you to follow up on the project.\"\\n\\nUser: \"What tasks do I have?\"\\nYou: [calls search with task filter] then \"Here are your current tasks:...\"\\n\\nCRITICAL: After you call a tool, you MUST write text. The tool result contains instructions - FOLLOW THEM and generate a response for the user. If you see \"Please summarize these findings for the user\" - DO IT IMMEDIATELY.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about Initial menu planning and pricing strategy<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nBased on my knowledge:\\n\\n**'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_rD6fl8EUxnG",
        "outputId": "e0460f63-d9b3-4bec-c4eb-0a29f6dbe8f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Based on my knowledge:\\n\\n**'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enWUM0jV-jV"
      },
      "source": [
        "We can see the System and Instruction prompts are successfully masked!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "64ae062f-1bfb-4e3d-f982-046a02c09c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "5.316 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "7ae88dd6-8814-4409-fd2e-2aa0e4eeb122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 360 | Num Epochs = 2 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 194,510,848 of 3,407,260,672 (5.71% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 22:29, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.317900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.964500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.124300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.075900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.998600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.576400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.923700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.343600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.263000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.228900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.144800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.391200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.317800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.707500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.122900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.608700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.158300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.092400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.158400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.155800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.326700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.178000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.115700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.047300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.030300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.074600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.008400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.088800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.004500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.190500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.024500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.054100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.366400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.004900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.004000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.028400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.007600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.008200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.008400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "9ce53509-0f7d-48c3-c3dd-d31eb7032bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1369.4234 seconds used for training.\n",
            "22.82 minutes used for training.\n",
            "Peak reserved memory = 8.014 GB.\n",
            "Peak reserved memory for training = 2.698 GB.\n",
            "Peak reserved memory % of max memory = 54.365 %.\n",
            "Peak reserved memory for training % of max memory = 18.303 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "5ed6b685-38e6-4e77-e29e-7a65a76308bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat marketing tasks did we do?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nBased on my knowledge:\\n\\n1. **Social Media Management**: Created and scheduled posts for various social media platforms, including Facebook, Twitter, Instagram, and LinkedIn. Monitored engagement, responded to comments, and adjusted content strategy based on performance.\\n\\n2. **Email Marketing**: Designed, wrote, and sent newsletters to subscribers,']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What marketing tasks did we do?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
        "                         temperature = 0.7, min_p = 0.1)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "bf963105-4f5b-476e-af27-140648612797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's the next few numbers in the Fibonacci sequence:\n",
            "\n",
            "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144\n",
            "\n",
            "The sequence continues by adding the previous two numbers to get the next number:\n",
            "\n",
            "- Start with 1\n",
            "- 1 + 1 = 2\n",
            "- 1 + 2 = 3\n",
            "- 2 + 3 = 5\n",
            "- 3 + 5 = 8\n",
            "- 5 + 8 = 13\n",
            "- 8 + 13 = 21\n",
            "-\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "3e2c0b30-9e42-44e7-85fa-6baaf89ecf3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/chat_template.jinja',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "148a4525-fae6-4873-9447-226356d10bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a description of the Eiffel Tower:\n",
            "\n",
            "The Eiffel Tower is an iconic and imposing iron lattice structure standing tall at the heart of Paris, the capital of France. \n",
            "\n",
            "The tower stands approximately 324 meters (1,063 feet) high, making it a recognizable and dominant feature in the city's skyline.\n",
            "\n",
            "Its lattice design, with four main pillars anchored to the ground, supports an intricate web of beams, struts, and girders that create the tower's four main sections: the first and second, or main, pillars; the lower, second level; and the upper, fourth level.\n",
            "\n",
            "Its height,\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWvy_dfLlnIh"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}